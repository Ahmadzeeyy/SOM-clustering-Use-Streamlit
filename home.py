# -*- coding: utf-8 -*-
"""SKRIPSIII-Fix2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V0jXzVONg38WeADibxBOaNqzpT_N7mB2
"""

import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from minisom import MiniSom
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
import seaborn as sns

st.set_page_config(page_title="Clustering Indikator Kesejahteraan Rakyat Jawa Timur ", layout="wide", initial_sidebar_state="collapsed")
st.title("Clustering Indikator Kesejahteraan Rakyat Jawa Timur")
uploaded_file = st.file_uploader("Upload data", type=None) 

list_of_tabs = ["Data View","Preprocessing", "Deteksi Outlier","penerapan algoritma SOM", "Validasi Hasil Cluster", "Algoritma SOM dari parameter terbaik" ]
whitespace = 9
# preprocessing, som_initiate, testing, som_initiate4best_parameters = st.tabs(["preprocessing", "penerapan algoritma SOM", "Validasi Hasil Cluster", "algoritma SOM dari parameter terbaik" ]) 
data_view, preprocessing, outlier_detection,  som_initiate_tab , testing, som_initiate4best_parameters = st.tabs([s.center(whitespace,"\u2001") for s in list_of_tabs])



best_parameters = []
best_parameters_each_indicator = []
# som inititiate for all data
def som_initiates(epoch, learning_rate,grid):
    global cluster_resuls_for_all_data, clusters
    data = df_clean[numerical].to_numpy()
    som = MiniSom(x=1, y=grid, input_len=data.shape[1], sigma=1.0, learning_rate=learning_rate, random_seed=42)
    som.random_weights_init(data)
    som.train_random(data, num_iteration=epoch)
    weights = som.get_weights().reshape(-1, data.shape[1])  # Ubah grid ke array 2D
    clusters = []
    for x in data:
        winner = som.winner(x)  # Koordinat unit pemenang
        winner_idx = winner[0] * grid + winner[1]  
        clusters.append(winner_idx+1)
    # Evaluasi menggunakan Silhouette Coefficient
    silhouette = silhouette_score(data, clusters)
    # Visualisasi Scatter Plot menggunakan PCA untuk reduksi dimensi
    pca = PCA(n_components=2)
    data_2d = pca.fit_transform(data)
    fig,ax = plt.subplots(figsize=(8, 5))
    ax.scatter(data_2d[:, 0], data_2d[:, 1], c=clusters, s=50)
    ax.set_title('Scatter Plot Penyebaran Data dengan  Cluster SOM')
    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    st.pyplot(fig)
    result = df_clean.copy()  # Duplikat data awal
    result['Cluster'] = clusters 
    cluster_membership = result[['KAB', 'Cluster']]
    cluster_membership = cluster_membership.sort_values(by='Cluster').reset_index(drop=True)
    # Tampilkan hasil
    st.write(f"Silhouette Coefficient : {silhouette:.4f}")
    cluster_resuls_for_all_data = cluster_membership
    return cluster_membership

# silhouette for best cluster usee all data 
def sillhoute_4best_cluster_alldata(epoch, learning_rate):
    iteration_cluster = range(2,11)
    score_clutser_results = []
    for i in  iteration_cluster:
      som_shape = (1, i)
      som = MiniSom(x=som_shape[0], y=som_shape[1], input_len=data.shape[1], sigma=1.0, learning_rate=learning_rate, random_seed=42)
      som.random_weights_init(data)
      som.train_random(data, num_iteration=epoch)
      clusters = []
      for x in data:
        winner = som.winner(x)
        winner_idx = winner[0] * som_shape[1] + winner[1]
        clusters.append(winner_idx+1)
      silhouette_avg = silhouette_score(data, clusters)
      score_clutser_results.append(silhouette_avg)
    fig,ax = plt.subplots(figsize=(8, 5))
    ax.plot(range(2, 11), score_clutser_results, marker='o', linestyle='--')
    ax.set_xlabel('Number of Clusters')
    ax.set_ylabel('silhouette coeficient')
    ax.set_title('silhouette coeficient')
    ax.grid(True)
    st.pyplot(fig)
    max_score = max(score_clutser_results)
    best_parameters.append([epoch,learning_rate,score_clutser_results.index(max_score)+2])

def som_initiate_data_each_indicator(epoch, learning_rate, grid):
        global cluster_resuls_for_each_indikator
        # results = []  # Untuk menyimpan hasil clustering dan evaluasi
        best_clusters = []  # Untuk menyimpan informasi anggota cluster dari setiap indikator
        data = df_clean
        average_score = []
        for indikator, variabel in indikator_variabel_map.items():
            st.write(f"\nScatter plot Indikator {indikator}")

            # Ambil subset data berdasarkan variabel
            subset_data = data[variabel].to_numpy()

            # Inisialisasi SOM
            som = MiniSom(
                x=1, y=grid, input_len=subset_data.shape[1],
                sigma=1.0, learning_rate=learning_rate, random_seed=42
            )
            som.random_weights_init(subset_data)
            som.train_random(subset_data, num_iteration=epoch)

            # Tetapkan setiap data ke cluster berdasarkan unit SOM terdekat
            clusters = []
            for x in subset_data:
                winner = som.winner(x)
                winner_idx = winner[0] * grid + winner[1]
                clusters.append(winner_idx)

            # Evaluasi Silhouette Coefficient
            silhouette_avg = silhouette_score(subset_data, clusters)
            average_score.append(silhouette_avg)

            # Simpan hasil clustering ke DataFrame
            cluster_col_name = f"Cluster_{indikator}"
            data[cluster_col_name] = clusters

            # Reduksi dimensi untuk visualisasi
            pca = PCA(n_components=2)
            data_2d = pca.fit_transform(subset_data)

            # Visualisasi scatter plot
            fig,ax = plt.subplots(figsize=(8, 4))
            ax.scatter(
            data_2d[:, 0], data_2d[:, 1], c=clusters, cmap='viridis', s=50)
            ax.set_title(f"{indikator} - Clustering Visualisation")
            ax.set_xlabel("PC1")
            ax.set_ylabel("PC2")
            ax.grid(True)
            st.pyplot(fig)
            # Ambil anggota cluster
            for cluster_id in sorted(data[cluster_col_name].unique()):
                anggota = data[data[cluster_col_name] == cluster_id]["KAB"].tolist()
                best_clusters.append({
                    "Indikator": indikator,
                    "Cluster": cluster_id + 1,  # Tambahkan 1 agar cluster dimulai dari 1
                    "Jumlah Anggota": len(anggota),
                    "Anggota": anggota
                })
        # Simpan hasil ke DataFrame
        best_clusters_df = pd.DataFrame(best_clusters)
        st.write(f"Rata-rata Silhouette Coefficient: {sum(average_score)/len(average_score):.4f}")
        cluster_resuls_for_each_indikator = best_clusters
        return best_clusters_df

def Sillhouette_4best_cluster_each_indicator (epoch, learning_rate):
        # Inisialisasi daftar untuk menyimpan hasil rata-rata Silhouette Coefficient
      iteration_cluster = range(2, 11)
      silhouette_scores = []

      # Iterasi untuk tiap jumlah klaster
      for i in iteration_cluster:
          silhouette_scores_per_indicator = []

          # Iterasi untuk tiap indikator
          for indikator, variabel in indikator_variabel_map.items():
              # Subset data untuk indikator saat ini
              subset_data = df_clean[variabel].to_numpy()

              # Inisialisasi dan pelatihan SOM
              som_shape = (1, i)  # Ukuran SOM (1 x jumlah klaster)
              som = MiniSom(x=1, y=i, input_len=subset_data.shape[1], sigma=1.0, learning_rate=learning_rate, random_seed=42)
              som.random_weights_init(subset_data)
              som.train_random(subset_data, num_iteration=epoch)

              # Menentukan klaster untuk tiap data
              clusters = []
              for x in subset_data:
                  winner = som.winner(x)
                  winner_idx = winner[0] * som_shape[1] + winner[1]  # Mengonversi 2D menjadi indeks klaster
                  clusters.append(winner_idx + 1)  # Klaster dimulai dari 1

              # Evaluasi dengan Silhouette Coefficient
              silhouette_avg = silhouette_score(subset_data, clusters)
              silhouette_scores_per_indicator.append(silhouette_avg)
          # Hitung rata-rata Silhouette Coefficient untuk jumlah klaster saat ini
          average_silhouette = sum(silhouette_scores_per_indicator) / len(silhouette_scores_per_indicator)
          silhouette_scores.append(average_silhouette)

      # Visualisasi hasil rata-rata Silhouette Coefficient
      fig,ax = plt.subplots(figsize=(8, 5))
      ax.plot(iteration_cluster, silhouette_scores, marker='o', linestyle='--', label="Average Silhouette Score")
      ax.set_xlabel('Number of Clusters')
      ax.set_ylabel('Silhouette Coefficient')
      ax.set_title('Average Silhouette Coefficient Across All Indicators')
      ax.grid(True)
      st.pyplot(fig)
      max_score = max(silhouette_scores)
      best_parameters_each_indicator.append([epoch,learning_rate,silhouette_scores.index(max_score)+2])

def heatmap_diagrams(clusters):
    # Pilih hanya kolom 21-30
    selected_columns = df_clean.columns[21:31]  # Ambil nama kolom dari index 20-29
    selected_features = df_clean[selected_columns].copy()

    # Gabungkan dengan hasil cluster
    selected_features["Cluster"] = clusters

    # Hitung rata-rata per cluster
    cluster_means = selected_features.groupby("Cluster").mean()

    # Plot heatmap
    fig,ax = plt.subplots(figsize=(8, 5))
    sns.heatmap(cluster_means.T, cmap="coolwarm", annot=True, fmt=".2f", cbar=False, linewidths=0.5, ax=ax)
    ax.set_title("Heatmap Distribusi Variabel dalam Setiap Cluster")
    ax.set_xlabel("Cluster")
    ax.set_ylabel("Variabel")
    st.pyplot(fig)


if uploaded_file is not None:
    outlier_detections = False 
    with data_view:
        st.header("Persiapan Data")
        df = pd.read_excel(uploaded_file,sheet_name='DATA2')
        col_name = ['JP', 'PPJK', 'JPM', 'PAMF', 'RLS', 'TPT', 'AK', 'RRP', 'ATAL', 'ATSL']
        df.columns = ['KAB'] + [f"{col_name[i % len(col_name)]}_{i // len(col_name) + 1}" for i in range(len(df.columns) - 1)]
        df
    with preprocessing:
        numerical = df.select_dtypes(include=['int64', 'float64']).columns
        df[numerical] = MinMaxScaler().fit_transform(df[numerical])
        st.subheader("Data Setalah Standarisasi dengan Min-Max Scaller")
        df
        numerical = df.select_dtypes(include=['int64', 'float64']).columns
        # st.success("berhasil menghapus variabel IPM")
    with outlier_detection:
        with st.sidebar:
            st.sidebar.title("Parameters Setting")
            values_outlier = st.slider("Pilih ambang Outlier data", 0.01, 0.1)   
        # Deteksi outlier menggunakan Isolation Forest
        df_scaled = df[numerical]
        clf = IsolationForest(contamination=values_outlier, random_state=70)
        clf.fit(df_scaled)
        y_pred = clf.predict(df_scaled)
        # Identifikasi outlier
        outlier_index = np.where(y_pred == -1)[0]
        df_clean = df.drop(outlier_index)
        st.write(f"Jumlah outlier yang terdeteksi: {len(outlier_index)}")
        outliers_df = df[y_pred == -1]
        st.write("Dataframe Outlier:")
        outliers_df
        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(df_scaled)
        data = df_clean[numerical].to_numpy() 
        # Visualisasi dalam satu plot dengan subplot
        fig, ax = plt.subplots(figsize=(8, 5))
        inliers = y_pred == 1
        outliers = y_pred == -1
        ax.scatter(pca_result[inliers, 0], pca_result[inliers, 1], color='blue')
        ax.scatter(pca_result[outliers, 0], pca_result[outliers, 1], color='red', label='Outlier')
        ax.set_xlabel('PC1')
        ax.set_ylabel('PC2')
        ax.set_title('Scatter Plot data')
        ax.legend()
        st.pyplot(fig)
        st.subheader("Dataframe setelah mengeliminasi Outlier : ")
        st.dataframe(df_clean,use_container_width=True)
    with som_initiate_tab :
        data_columns = df_clean.columns[1:]
        indikator_variabel = {
        "Kependudukan": ["JP"],
        "Kesehatan dan Gizi": ["PPJK"],
        "Pendidikan": ["PAMF", "RLS"],
        "Ketenagakerjaan": ["AK", "TPT"],
        "Taraf dan Pola Konsumsi": ["RRP"],
        "Perumahan": ["ATAL", "ATSL"],
        "Kemiskinan": ["JPM"]}
        indikator_variabel_map = {
        indikator: [col for col in data_columns if col.split('_')[0] in indikator_variabel[indikator]]
        for indikator in indikator_variabel
        }

        data_for_use_som= st.selectbox("Pilih Data yang Ingin Digunakan :  ", ("Seluruh Data ", "Tiap Indikator"), key="for SOM")
        st.subheader("Parameter SOM setting")
        col1, col2,col3 =st.columns([3,3,1], vertical_alignment="top", border=True)
        col2tab1,col2tab2 = st.columns(2,vertical_alignment="top")
        col4results1,col4results2,col4results3 =st.columns(3)
        som_Learning_rate = col1.slider("Pilih Nilai Learning rate ", 0.01,0.1, key="Som-parameter-2")
        som_epoch= col2.slider("Pilih Nilai Epoch ", 1000,10000,step=1000,key="Som-parameter-1")
        som_cluster= col3.selectbox("Pilih Jumlah Cluster ", (2,3,4,5,6,7,8,9))
        # Input data
        if data_for_use_som == "Seluruh Data " :
            with col2tab1 :
                st.write("visualiasi hasil klaster ")
                som_initiates(som_epoch, som_Learning_rate, som_cluster )
            with col2tab2 :
                st.write(f"Performa SOM dengan, learning rate ={som_Learning_rate}, dan epoch ={som_epoch}")
                sillhoute_4best_cluster_alldata(som_epoch, som_Learning_rate)
        elif "Tiap Indikator" : 
            with col2tab1 :
                som_initiate_data_each_indicator(som_epoch, som_Learning_rate, som_cluster )
            with col2tab2 :
                st.write(f"Performa SOM pada klaster ={som_cluster}, learning rate ={som_Learning_rate}, dan epoch ={som_epoch}")
                Sillhouette_4best_cluster_each_indicator(som_epoch, som_Learning_rate)

        # som_initiate_data_each_indicator(2000, 0.05, 3)
        
    with testing:
        data_for_use= st.selectbox("Pilih Data yang Ingin Digunakan :  ", ("Seluruh Data", "Data Tiap Indikator"),key="for testing-222")
        col4testing1,col4testing2 = st.columns(2)
        if data_for_use == "Seluruh Data" :
            with col4testing1 :
                st.subheader("Skenario Uji Coba 1 ")
                st.write("seknario uji coba dengan som parameter Epoch = 5000 dan learning rate = 0.05 ")
                sillhoute_4best_cluster_alldata(5000, 0.05)
            with col4testing2 :
                st.subheader("Skenario Uji Coba 3 ")
                st.write("seknario uji coba dengan som parameter Epoch = 5000 dan learning rate = 0.1 ")
                sillhoute_4best_cluster_alldata(5000, 0.1)
        elif data_for_use == "Data Tiap Indikator": 
            with col4testing1 :
                st.subheader("Skenario Uji Coba 1 ")
                st.write("seknario uji coba dengan som parameter Epoch = 5000 dan learning rate = 0.05 ")
                Sillhouette_4best_cluster_each_indicator(5000, 0.05)
            with col4testing2 :
                st.subheader("Skenario Uji Coba 4 ")
                st.write("seknario uji coba dengan som parameter Epoch = 5000 dan learning rate = 0.1 ")
                Sillhouette_4best_cluster_each_indicator(5000, 0.1)

    
    with som_initiate4best_parameters:
        data_for_use_parameter= st.selectbox("Pilih Data yang Ingin Digunakan :  ", ("Seluruh Data", "Tiap Indikator"),key="for best parameter")
        col_4best_parameter1,col_4best_parameter2 = st.columns([5,3])
        if data_for_use_parameter == "Seluruh Data" and len(best_parameters) > 1 :
            with col_4best_parameter1 :
                st.write("visualiasi hasil klaster Uji Skenario 1")
                col_4best_parameter2.write(f"Epoch = { best_parameters[(len(best_parameters)-2)][0]}, Learning rate = {best_parameters[(len(best_parameters)-2)][1]}, jumlah cluster = {best_parameters[(len(best_parameters)-2)][2] }")
                som_initiates(best_parameters[(len(best_parameters)-2)][0], best_parameters[(len(best_parameters)-2)][1], best_parameters[(len(best_parameters)-2)][2] )
                heatmap_diagrams(clusters)
                col_4best_parameter2.dataframe(cluster_resuls_for_all_data, use_container_width=True)
                st.write("visualiasi hasil klaster Uji Skenario 3")
                som_initiates(best_parameters[(len(best_parameters)-1)][0], best_parameters[(len(best_parameters)-1)][1], best_parameters[(len(best_parameters)-1)][2] )
                heatmap_diagrams(clusters)
                
                col_4best_parameter2.write(f"Epoch = { best_parameters[(len(best_parameters)-1)][0]}, Learning rate = {best_parameters[(len(best_parameters)-1)][1]}, jumlah cluster = {best_parameters[(len(best_parameters)-1)][2] }")
                col_4best_parameter2.dataframe(cluster_resuls_for_all_data, use_container_width=True)
        elif data_for_use_parameter == "Tiap Indikator" and  len(best_parameters_each_indicator) > 1 : 
           st.write(best_parameters_each_indicator)
           with col_4best_parameter1 :
                col_4best_parameter2.write(" ")
                col_4best_parameter2.write(f"Epoch = { best_parameters_each_indicator[(len(best_parameters_each_indicator)-2)][0]}, Learning rate = {best_parameters_each_indicator[(len(best_parameters_each_indicator)-2)][1]}, jumlah cluster = {best_parameters_each_indicator[(len(best_parameters_each_indicator)-2)][2] }")
                som_initiate_data_each_indicator(best_parameters_each_indicator[(len(best_parameters_each_indicator)-2)][0], best_parameters_each_indicator[(len(best_parameters_each_indicator)-2)][1], best_parameters_each_indicator[(len(best_parameters_each_indicator)-2)][2] )
                col_4best_parameter2.dataframe(cluster_resuls_for_each_indikator, use_container_width=True)
                
                som_initiate_data_each_indicator(best_parameters_each_indicator[(len(best_parameters_each_indicator)-1)][0], best_parameters_each_indicator[(len(best_parameters_each_indicator)-1)][1], best_parameters_each_indicator[(len(best_parameters_each_indicator)-1)][2] )
                col_4best_parameter2.write(f"Epoch = { best_parameters_each_indicator[(len(best_parameters_each_indicator)-1)][0]}, Learning rate = {best_parameters_each_indicator[(len(best_parameters_each_indicator)-1)][1]}, jumlah cluster = {best_parameters_each_indicator[(len(best_parameters)-1)][2] }")
                col_4best_parameter2.dataframe(cluster_resuls_for_each_indikator, use_container_width=True)